[{"authors":["admin"],"categories":null,"content":"I am a Researcher at the Research and Innovation Division of Tata Consultancy Services (TCS), where I work with Dr. Harshad Khadilkar and Dr. Gautam Shroff in Data and Decision Sciences Group since May 2018. Before this, I was associated with Dr. Lipika Dey for two years in the Text and Data Mining Group.\nMy Research interests include:\n Intersection of Deep learning and Natural Language processing Application of Reinforcement learning on practical control and decision problems Brain computer interface and its applications in rehabilitation  Before joining TCS, I was a student at Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India pursuing Master of Technology (M.Tech.) with Machine Intelligence specialization. I was supervised by Prof. Nagraj Ramrao and co-supervised by Prof. Suman Mitra. My External Supervisors were from Nanyang Technological University Prof. Suresh Sundaram and Prof. N. Sundararajan.\nApart from research work, I’m either working on my pet projects, fiddling with hardware such as Rpi, playing chess or watching video essay\u0026rsquo;s on cinema.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Researcher at the Research and Innovation Division of Tata Consultancy Services (TCS), where I work with Dr. Harshad Khadilkar and Dr. Gautam Shroff in Data and Decision Sciences Group since May 2018.","tags":null,"title":"Hardik Meisheri","type":"authors"},{"authors":null,"categories":null,"content":"Here are list of blogs are informative for RL/ML\n  Machine Learning Blog at CMU  Berkeley Artificial Intelligence Research Blog  Standford AI Blog  Lilian Weng blog on RL  Deep Mind Blog  ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"18fa5204aeaa0f71f3e322f6df5c7d92","permalink":"/post/2020-09-21-interesting_blogs/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/post/2020-09-21-interesting_blogs/","section":"post","summary":"Here are list of blogs are informative for RL/ML\n  Machine Learning Blog at CMU  Berkeley Artificial Intelligence Research Blog  Standford AI Blog  Lilian Weng blog on RL  Deep Mind Blog  ","tags":["Blogs","ML"],"title":"AI/robotics Blog list","type":"post"},{"authors":["Sachin Thukral","Arnab Chatterjee","Hardik Meisheri","Tushar Kataria","Aman Agarwal","Ishan Verma","Lipika Dey"],"categories":null,"content":"","date":1577491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577491200,"objectID":"eeaaf2a41584b8b712c531091a122801","permalink":"/publication/characterizing_behavioral_trends_in_a_community_driven_discussion_platform/","publishdate":"2019-12-28T00:00:00Z","relpermalink":"/publication/characterizing_behavioral_trends_in_a_community_driven_discussion_platform/","section":"publication","summary":"This article presents a systematic analysis of the patterns of behavior of individuals as well as groups observed in community-driven platforms for discussion like Reddit, where users usually exchange information and viewpoints on their topics of interest. We perform a statistical analysis of the behavior of posts and model the users’ interactions around them. A platform like Reddit which has grown exponentially, starting from a very small community to one of the largest social networks, with its large user base and popularity harboring a variety of behavior of users in terms of their activity. Our work provides interesting insights about a huge number of inactive posts which fail to attract attention despite their authors exhibiting Cyborg-like behavior to attract attention. We also observe short-lived yet extremely active posts emulate a phenomenon like Mayfly Buzz. A method is presented, to study the activity around posts which are highly active, to determine the presence of Limelight hogging activity. We also present a systematic analysis to study the presence of controversies in posts. We analyzed data from two periods of 1-year duration but separated by few years in time, to understand how social media has evolved through the years.","tags":["Behavioral Analysis","Reddit"],"title":"Characterizing Behavioral Trends in a Community Driven Discussion Platform","type":"publication"},{"authors":null,"categories":null,"content":"I presented my work on Pommerman at Deep Reinforcement Learning Workshop in NeurIPS this year, titled \u0026ldquo;Accelerating training in Pommerman with Imitation and Reinforcement Learning\u0026rdquo;, coauthors are Omkar Shelke, Richa Verma and Harshad Khadilkar. The selection of talks and posters to be visited was done primarily for Reinforcement Learning and to some extent NLP. Due to high rush during the poster session (one has to wait in queue for 30 minutes to enter poster session) and 4 parallel tracks during presentation, I have most probably seen 20-25% of the conference session. The main highlight for me was the socials which were introduced this year by NeurIPS for having informal discussions with prominent people in the field. I attended RL social and had a detailed discussion with Richard Sutton, David Silver, Martha White, Michael Littman etc, over topics ranging from causality in RL to going away with the MDP settings in the RL. All in all, it was an enlightening experience to have a deep philosophical discussion, especially with Sutton. The common notion that resonated with all of them was to have a big picture in mind, before delving down to a very specific problem. If building an AGI is the ultimate goal, try to place your work in that context and work from there.\nIn RL, people have been looking at sample efficient RL, batch RL, meta-learning and towards ablation studies of existing algorithms. It would not be correct to gauge the overall direction of community as I did not attend all of the talks, going through the papers and posters I did find that lot of work has been towards having much more deeper understanding towards the theory of deep learning. Talks and most of the workshops were recorded and recordings are available at https://slideslive.com/neurips/ . This report is short summary of some of the talks/tutorials I attended, and a list of things that I found interesting during the conference.\n Tutorials Imitation learning and its application to Natural Language Generation   Given by: Kyunghyun Cho, Hal Daume III \nThe tutorial focuses on using imitation learning and reinforcement learning in NMT (Neural Machine Translation), dialogues and story generation. The main challenge in the using Beam search for searching is lack of diversity and in the literature mostly it is tackled by adding noise. Reinforcement learning with its stochastic policies can have a great improvement here, especially when we want to have more natural dialogues, stories which are not similar yet have a structure of plots and make sense etc. The story examples given by the authors were really impressive.\nEfficient Processing of Deep Neural Network: from Algorithm to Hardware  Given by: Vivienne Sze \nThe tutorial provides insights into designing efficient hardware for DL. They focus on different constraints such as speed, latency, energy consumption and cost. All of these constraints require a careful trade-off between each other. For example as shown in the figure below, where we can increase the speed of convolution at the expense of data duplication. They discuss architecture such as CPU and GPU to task-specific architectures of FPGA. Most of the presentation was focused on optimizing the hardware for Image processing and vision task.\nReinforcement Learning: Past, Present and Future Perspectives Given by: Katja Hofmann This was a very elaborate session of RL, from basic MDP formulation to Multi-agent RL with a different perspective in generalization and evaluating policies. Minecraft case study was very interesting for long term reward and exploration.\nInvited Talks/Keynotes Celeste Kidd: How to know  Questions: \n How do we know, what we know How do two people living in the same world come to believe very different things Why do people believe somethings which aren\u0026rsquo;t true Age of information and also misinformation How do we form Belief Vast amount of knowledge present in the world, an individual learner has to pick and choose what to learn and how to learn, how does one decide that  We need to answer all these questions because we will need to build much more accurate models of the world and have this knowledge embedded in the AI agents. Her work is around mostly inferencing on what babies infer in the real world, what do they look, where to they focus, what kind of prediction they make. Similar work was discussed by Josh Tannenbaum in his last year\u0026rsquo;s ICML talk\n Five Key things \n Humans continuously form beliefs (It\u0026rsquo;s not a one-step process) \u0026ndash;\u0026gt; think of it as a probabilistic expectation. (Humans tend to lose attention if things are too predictable or too surprising, picking a book which one knows (redundancy) is boring to read whereas picking a book written in the language which you don\u0026rsquo;t even understand (absolute new information) is also not fun to read) Certainty diminishes interest, an agent should be smart enough to not learn and waste resources on which it is certain about, humans do that. However, we are often wrong about things which we are certain about. We have a very poor model of our own uncertainty. Certainty is driven by feedback. When feedback is not available our feedback might be way off. Less feedback may encourage overconfidence. Small confirmatory feedback is enough to solidify some concepts and lead to wrong assumptions about concepts. Given the diversity in the concepts for the same object, are people aware of the weird behaviour about the notion of the concept that they have. Humans form beliefs quickly. The algorithms pushing content online have profound impacts on what we believe.  She talked about the work toxicity for women and how small instances of false accusation has created a fear among men. She emphasized when women complains and talks about some bad things that she has gone through, chances are she as went through much more than that. Standing Ovation by the crowd.\nTest of time, Lin Xiao: Dual Averaging Method for Regularized Stochastic Learning and Online Optimization Two main ideas which were dominant/promising direction in 2009, Stochastic Gradient Descent and Online Convex Optimization. Other important works during early 2000\u0026rsquo;s compressed sensing/sparse optimization, interior point methods, proximal gradient methods. Paper explores combining SGD with sparse optimization.\nYoshua Bengio: From System 1 Deep Learning to System 2 Deep Learning  Motivation \nQuestions: is it enough to have more data, compute, large models? How far are we from Human-level AI. We need to think more tangential to the DL paradigm. Inspiration: “Thinking Fast and Slow” by Daniel Kahneman. System-1,2 is also same as describe in podcast of Sean Carroll System 1 \u0026ndash;\u0026gt; Intuitive, fast unconscious non-linguistic, and habitual decision making/inference. (DL is good at this) System 2 \u0026ndash;\u0026gt; Slow, logical, sequential, conscious, linguistic, algorithmic,decision making. (DL is not equipped to deal with this)\n Things missing in DL \n Out of distribution generalization and transfer High level cognition: need to move from system 1 to system 2 \u0026ndash;\u0026gt; High level semantic representation, causality Machines that understand the world by building world models, Knowledge seeking mechanism   Talk \n ML Goal: Handle Changes in Distribution System 2 Basics: Attention and Consciousness Consciousness Prior: Sparse Factor Graph Theoretical framework: meta-learning, localized change hypothesis causal discovery Compositional DL architectures  Michael Littman on Assessing the Robustness of Deep RL Algorithms Motivation for this talk was to understand, why any decisions are taken by a policy in certain scenarios (DQN for atari as example), build a Saliency map by masking of portions of state space. Conclusion being, DQN does not learn the statespace as we see it, although it has strategies to win the game it does not understand the game per say. Evaluation metrics for generalization Value Estimation Error, total accumulated reward.\nOral/spotlight Presentation Towards Explaining the regularization effect of initial large learning rate in training neural networks Authors explain the effect of learning rate on patterns that are learnt. They prove that having a lower learning rate learns very myopic structure, where as larger learning rate learns the overall/macro structure. They prove that over a CIFAR dataset, where they super-impose a patch of image over the original image in the dataset. The Image from the dataset can be considered to hard-to-generalize but easy-to-fit sample, whereas small patch can be considered as easy-to-generalize but hard-to-fit sample.\nSample Complexity of Deep Neural Networks via Lipschitz Augmentation Objective is to prove upper bounds on generalization error and improve with regularization. The norm of weights is not sufficient for getting upper bounds, instead make it is a function of data.\nOutstanding New Directions Paper: Uniform Convergence may be unable to explain generalization in deep learning Objective of this work was to understand why does over-parameterized networks generalize well. The generalization error or the bound does not just depend on the parameters but also on the training set size. Uniform convergence does not explain the generalization.\nOn Exact Computation with an Infinitely Wide Neural Net Work was based on relationship between the width of the Neural network and neural tangent kernel. On the lines of understanding larger width networks can achieve zero prediction errors. NTK performance is comparable to CNNs.\nGeneralization Bounds of SGD for Wide and Deep NNs Key ideas: Deep ReLu nets are almost linear in parameters in a small neighborhood around initialization, Loss is Lipschitz continuous and almost convex\nCausal Confusion in Imitation Learning Imitation learning: powerful mechanism for learning complex behaviors. Behavioral cloning is imitation learning with supervised settings. But the imitation policy fails the moment it encounters state which it has not seen during behavior cloning, and there is catastrophic degradation in the policies (Distributional shift in the data). Does more information mean better performance \u0026ndash;\u0026gt; Not necessarily\nBehavior cloning learns to brake when the brake light is on!!!! Just having the information about the road is more generalized. Giving more information leads to confusion. Ways to tackle is predict actions conditioned over causes and Targeted intervention by expert queries.\nImitation Learning from Observations by Minimizing Inverse Dynamics Disagreement This work was on imitation learning when you need to learn the expert policy with just state/observations. They argue that the difference between the imitation learning and learning with just state spaces is proportional in the disagreement of actions in the actions and expert (GAIL).\nLearning to Control Self-Assembling Morphologies: A study of Generalization via Modularity Authors propose to have lego kind of modules with each module taking inputs+message and generating outputs + message. This settings can lead to much more robust policies in co-operative environment. Co-Evolution of control and morphology. Policies are shared along the modules and hence they learn much more robust policies. Link to the video https://youtu.be/cg-RdkPtRiQ, last part of the video where, it accomplishes the task even with losing some modules is spooky. A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning Objective: Find policies that generalize to new agent-task pairs not seeing during training. Compute score on previous pairs, using structured inference, use this to infer new policy on new pairs. Add a new constraint to the optimization to make it a linear program. The constraint encourages agents to cooperate. Further extend the LP to a quadratic program.\nGuided Meta-Policy Search RL is very sample inefficient. Meta-Learning is one possible solution to this problem by making efficent use of previous training tasks. Most meta-RL methods also requires huge amounts of data for the training phase, even if the adaptation is efficient. Also shortcomings in exploration and credit assignment during the training phase.\nSolution, train on each task separately. Then train a meta-policy in a supervised manner from earlier policies. Using a Logarithmic Mapping to enable Lower Discount Factors in Reinforcement Learning For Infinite horizon, the gamma decays exponentially. Therefore most of the gammas need to be near 1 to be meaningful for infinite horizon tasks. However, this makes learning hard. Experiments show that with low gamma policies are very bad. Action Gap is defined as difference in Q values for a given state between optimal action and action taken. They prove that with low discount factors the action gap is large.\nThey present the Log(Q) values instead of Q value Better Exploration with Optimistic Actor Critic Being too Greedy is bad. However, Policy gradient methods are greedy. Solution is to use Conservative update which reduces overestimation. However this leads to conservative policies which do not explore much. They provide upper bound to critic values instead of lower bound, which leads to optimistic estimation. They improve of SAC.\nRobust Exploration in Linear Quadratic Reinforcement Learning Robust Exploration $\\rightarrow$ does not cause catastrophic degradation of policy. Targeted Exploration $\\rightarrow$ knowledge which can improve the task solving capabilities. They formulate the exploration-exploitation problem into convex optimization problem.\nTight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies Minmax Regret bounds for Model-based RL based on short term and long term planning. They show that short-term planning (one step planning) is minmax optimal with finite horizon MDPs.\nHindsight Credit Assignment Usually, the credit assignment is done with temporal scale and assume that noisy assignment would eventually be able to learn the long term action-consequences. Man has to figure out what caused him to be wet during the day. As an RL algorithm, he would spend many days getting wet by not bringing an umbrella. They solve this by explicitly learning the relevant credit using posterior probabilities. Weight Agnostic Neural Networks Why are CNNs much better than Fully-connected layers. No matter how much you increase the width of FC layers it would not beat CNNs. The answer lies in the structure of CNNs. This works explores how much far just the structure of Neural Networks can take us without relying on the weights. They create a initial population of structures with connections and then initialize with different weights for each structure. Without training they evaluate the performance of all the structures and pick out top K candidates. Perform 5 additional operations such as addition of nodes, addition of connections etc and do the procedure repeatedly. They show that structure alone can lead to 82% accuracy over MNIST dataset without training.\nA neurally plausible model learns successor representations in partially observable environments This paper was about representation of belief states with he help of distributed distributional code and successor features. They derive the connection between these two from neuroscience. They also generalize this over POMDPs\nDualDICE: Behaviour-agnostic estimation of discounted stationary Distribution Corrections How can we estimate average discounted return for a particular policy for off policy algorithms as the experience collection is done over varied number of policies. They propose a change in the loss function based over ratio of density problem (similar to PPO) with importance of weighting trick.\nVIREL: A Variational Inference Framework for Reinforcement Learning Look at RL as probabilistic inference problem. People have tried Pseudo-likelihood methods (risk taking) and Maximum entropy objective (such as SAC, however they have issue of non-recoverable of optimal policies). They present actor-critic methods with expectation maximization with much better sample efficiency.\nPosters RL  Doubly-Robust Lasso Bandit - Gi Soo Kim, Myunghee Cho Paik - Seoul National University Decentralized cooperative stochastic bandits - David Martinez-Rubio, Varun Kanade, Patrick Rebeschini - Oxford Multi-agent Common knowledge Reinforcement Learning - Christian A. Schroeder de Witt, Jakob N. Foerster, Gregory Farquhar, Philip H. S. Torr, Wendelin Boehmer, Shimon Whiteson Meta-Learning with Warped Gradient Descent - Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Hujun Yin, Raia Hadsell Measuring the reliability of RL algorithms - Stephanie C.Y. Chan, Sam Fishman, John Canny, Anoop Korattikara, Sergio Guadarrama Adaptive Online Planning for Continual Lifelong Learning - Kevin Lu, Igor Mordatch, Pieter Abbeel Learning Efficient Representations for Intrinsic Motivation - Ruihan Zhao, Stas Tiomkin, Pieter Abbeel Harnessing Structures for value based planning and reinforcement learning - Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi Benchmarking Safe Exploration in DRL - Alex Ray, Joshua Achiam, Dario Amodei Search on the Replay Buffer: Bridging Planning and Reinforcement Learning  NLP  Landmark Ordinal Embedding - Nikhil Ghosh, Yuxin Chen, Yisong Yue Text based interactive recommendation via constraint-augmented reinforcement learning - Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen Model based Reinforcement learning with adversarial training for online recommendation - Xueying Bai, Jian Guan, Hongning Wang Stochastic shared Embeddings: Data Driven Regularization of Embedding Layers - Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James Sharpnack Quantum Embedding of Knowledge for Reasoning - Dinesh Garg, Shajith Ikbal, Santosh K. Srivastava, Harit Vishwakarma, Hima Karanam, L Venkata Subramaniam Can Unconditional Language Models Recover Arbitrary Sentences? - Nishant Subramani, Sam Bowman, Kyunghyun Cho Interpreting and Improving natural-language processing (in machine) with natural language-processing (in the brain) - Mariya Toneva, Leila Wehbe  Misc  ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for BlackBox Optimization - Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, David Cox High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks - Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, Honglak Lee Adversarial Examples are Not bugs, they are features - Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry Dynamic of stochastic gradient descent for two-layer neural networks in the teacher-student setup - Sebastian Goldt, Madhu S. Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová Poincare recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave zero-sum games- Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Georgios Piliouras Computational Separations between Sampling and Optimization - Kunal Talwar Learning Imbalanced Datasets with Label-distribution-aware Margin Loss - Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma vGraph: A Generative Model for Joint Community Detection and Node Representation Learning - Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, Jian Tang Putting an End to End-to-End: Gradient Isolated learning of Representations - Sindy Löwe, Peter O\u0026rsquo;Connor, Bastiaan S. Veeling  ","date":1576281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576281600,"objectID":"1261acdbfd0206c2f44eb32fd8a85e76","permalink":"/post/2019-12-14-neurips-2019/","publishdate":"2019-12-14T00:00:00Z","relpermalink":"/post/2019-12-14-neurips-2019/","section":"post","summary":"I presented my work on Pommerman at Deep Reinforcement Learning Workshop in NeurIPS this year, titled \u0026ldquo;Accelerating training in Pommerman with Imitation and Reinforcement Learning\u0026rdquo;, coauthors are Omkar Shelke, Richa Verma and Harshad Khadilkar.","tags":["Conference","ML"],"title":"NeurIPS-2019 Summary","type":"post"},{"authors":["Hardik Meisheri","Omkar Shelke","Richa Verma","Harshad Khadilkar"],"categories":null,"content":"","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"a598d7817657fbcb3d19ac7aa61421f9","permalink":"/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/","section":"publication","summary":"The Pommerman simulation was recently developed to mimic the classic Japanese game Bomberman, and focuses on competitive gameplay in a multi-agent setting. We focus on the 22 team version of Pommerman, developed for a competition at NeurIPS 2018. Our methodology involves training an agent initially through imitation learning on a noisy expert policy, followed by a proximal-policy optimization (PPO) reinforcement learning algorithm. The basic PPO approach is modified for stable transition from the imitation learning phase through reward shaping, action filters based on heuristics, and curriculum learning. The proposed methodology is able to beat heuristic and pure reinforcement learning baselines with a combined 100,000 training games, significantly faster than other non-tree-search methods in literature. We present results against multiple agents provided by the developers of the simulation, including some that we have enhanced. We include a sensitivity analysis over different parameters, and highlight undesirable effects of some strategies that initially appear promising. Since Pommerman is a complex multi-agent competitive environment, the strategies developed here provide insights into several real-world problems with characteristics such as partial observability, decentralized execution (without communication), and very sparse and delayed rewards.","tags":["Reinforcement Learning","Games","Pommerman","Deep Learning"],"title":"Accelerating Training in Pommerman with Imitation and Reinforcement Learning","type":"publication"},{"authors":["Hardik Meisheri","Vinita Baniwal","Nazneen N Sultana","Balaraman Ravindran","Harshad Khadilkar"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"c01fc249d82bd6e40bd64403177105ee","permalink":"/publication/reinforcement_learning_for_multi_objective_optimization_of_online_decisions_in_high_dimensional_systems/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/publication/reinforcement_learning_for_multi_objective_optimization_of_online_decisions_in_high_dimensional_systems/","section":"publication","summary":"This paper describes a purely data-driven solution to a class of sequential decision-making problems with a large number of concurrent online decisions, with applications to computing systems and operations research. We assume that while the micro-level behaviour of the system can be broadly captured by analytical expressions or simulation, the macro-level or emergent behaviour is complicated by non-linearity, constraints, and stochasticity. If we represent the set of concurrent decisions to be computed as a vector, each element of the vector is assumed to be a continuous variable, and the number of such elements is arbitrarily large and variable from one problem instance to another. We first formulate the decision-making problem as a canonical reinforcement learning (RL) problem, which can be solved using purely data-driven techniques. We modify a standard approach known as advantage actor critic (A2C) to ensure its suitability to the problem at hand, and compare its performance to that of baseline approaches on the specific instance of a multi-product inventory management task. The key modifications include a parallelised formulation of the decision-making task, and a training procedure that explicitly recognises the quantitative relationship between different decisions. We also present experimental results probing the learned policies, and their robustness to variations in the data.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Reinforcement Learning for Multi-Objective Optimization of Online Decisions in High-Dimensional Systems","type":"publication"},{"authors":["Souvik Barat","Harshad Khadilkar","Hardik Meisheri","Vinay Kulkarni","Vinita Baniwal","Prashant Kumar","Monika Gajrani"],"categories":null,"content":"","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"804b03e93ad5bfdf7bea76a8f316f8c0","permalink":"/publication/actor_based_simulation_for_closed_loop_control_of_supply_chain_using_reinforcement_learning/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/publication/actor_based_simulation_for_closed_loop_control_of_supply_chain_using_reinforcement_learning/","section":"publication","summary":"Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Actor Based Simulation for Closed Loop Control of Supply Chain using Reinforcement Learning","type":"publication"},{"authors":null,"categories":null,"content":"For a long time, I wanted to write a summary of the conference that I attended. Not just to share the new ideas and trends prevalent, also to keep track of the thoughts that I had during the conference. Writing has been one of my weakness, I cannot and do not want to write things down. I am too lazy to write down ideas, and because writing it down concertizes the idea, it loses its charm of being glamorous, just like as any tourist destination nowadays.\nEMNLP-2018 has been a huge conference this year, close to 2500 attendees and around 550 papers making it the largest NLP conference ever. It was by far one of the best conferences that I have attended.\nI wanted to keep track of all the notes and the papers that I would conveniently put in my to read folder, so I thought of writing a blog post on it to summarize the on going trend and the tricks most of paper used recently.\nBroad and quick summary News  EMNLP 2019 will be co-located with IJCNLP Hongkong (Expect Chinese to take over NLP soon) A lot of visa issue happened in Brussels too, one of the Ph.D. students was detained for 5–6 hrs and deported back, keynote speaker for workshop Dilip Rao gave a talk on skype. Next conferences NAACL- Minneapolis, ACL-florence All the talks have been recorded and videos might be up soon.  General trends across the conference  Almost all the papers report that ELMO (Stands for Deep contextualized word representations, won the best paper award in NAACL this year) performs way better than using GLOVE embeddings and most of NLP community has moved to language models. Bi-LSTM + Attention mechanism has been common go to architecture Good number on papers on languages with low representations, such as Spanish, Tamil etc using transfer learning, be it on representation, generation or translation Blackbox NLP was the new workshop organized this year and it features some of the best talks in the conference (Partially due to heavy weights supporting and publicizing it) Keynote by Yoav Goldberg discussed what are the downsides of using RNNs and what specifically it could encode and what it could not. Some papers that I liked - Interpretable Structure Induction via Sparse Attention, I was unaware with the sparse attention and apparently it was talked about in WMT, WASSA and Blackbox NLP a lot. - Understanding Convolutional Neural Networks for Text Classification Did some ablation study on what the CNN is focusing on while doing a sentiment analysis. Most of the time it just focuses on the random thing, they also tried augmenting data with the negation and flipping the labels, interesting paper to check it out.  Keynote talk III The Moment of Meaning and the Future of Computational Semantics by Johan Bos, talked about classical way of doing semantic analysis and some anecdotes on why BLEU is not the correct or rather only metric on which translation and generation should be compared (Simple negation can yield upto 0.9 score), argued that metric should be more rooted and should contain the aspect of what it means. Although his work could not surpass the neural semantic approach.\nBest papers  MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling might be useful for people working on chatbot, they have released a dataset containing dialogues over multiple domains and benchmarked using the current systems. Linguistically-Informed Self-Attention for Semantic Role Labeling, this was total awe aspiring talk, semantic role labeling with multi-headed attention and very deep network. (https://github.com/strubell/LISA) Phrase-Based \u0026amp; Neural Unsupervised Machine Translation, this is kind of neural stylistic transfer just as in image where they transfer artistic styles to the images. But in this, they transfer it across age, gender etc. Cool stuff.  ","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"c0cdc2774a61376b8271b18b73a5d47b","permalink":"/post/2019-01-26-emnlp-2018/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/post/2019-01-26-emnlp-2018/","section":"post","summary":"For a long time, I wanted to write a summary of the conference that I attended. Not just to share the new ideas and trends prevalent, also to keep track of the thoughts that I had during the conference.","tags":["Conference","NLP"],"title":"EMNLP-2018 Summary","type":"post"},{"authors":["Kaustubh Mani","Ishan Verma","Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"5648cfdc272d7f3d8a9635e8b131703f","permalink":"/publication/multi-document_summarization_using_distributed_bag-of-words_model/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/multi-document_summarization_using_distributed_bag-of-words_model/","section":"publication","summary":"As the number of documents on the web is growing exponentially, multi-document summarization is becoming more and more important since it can provide the main ideas in a document set in short time. In this paper, we present an unsupervised centroid-based document-level reconstruction framework using distributed bag of words model. Specifically, our approach selects summary sentences in order to minimize the reconstruction error between the summary and the documents. We apply sentence selection and beam search, to further improve the performance of our model. Experimental results on two different datasets show significant performance gains compared with the state-of-the-art baselines.","tags":["Summarization","Deep Learning","NLP"],"title":"Multi-Document Summarization using Distributed Bag-of-Words Model","type":"publication"},{"authors":["Ishan Verma","Rahul Ahuja","Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543795200,"objectID":"bf317c8078358c4894718342ed6fc5f7","permalink":"/publication/air_pollutant_severity_prediction_using_bi-directional_lstm_network/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/publication/air_pollutant_severity_prediction_using_bi-directional_lstm_network/","section":"publication","summary":"Air pollution has emerged as a universal concern across the globe affecting human health. This increasing danger motivates the study of systems for predicting air pollutant severities ahead of time. In this paper, we have proposed the use of a bi-directional LSTM model to predict air pollutant severity levels ahead of time. We have shown that the predictions can be significantly improved using an ensemble of three Bi-Directional LSTMs (BiLSTM) that model the long-term, short-term and immediate effects of PM2.5 (the key air pollutant) severity levels. Further, weather information data has been taken into account while modelling, since they are found to boost prediction accuracies. Experimental results for multiple locations in New Delhi, India are presented to demonstrate model superiority over earlier techniques.","tags":["Time Series Prediction","Deep Learning"],"title":"Air Pollutant Severity Prediction Using Bi-Directional LSTM Network","type":"publication"},{"authors":["Hardik Meisheri","Harshad Khadilkar"],"categories":null,"content":"","date":1540944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540944000,"objectID":"62d0019f656c1058a7abc331d075c23d","permalink":"/publication/learning_representations_for_sentiment_classification_using_multi-task_framework/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/learning_representations_for_sentiment_classification_using_multi-task_framework/","section":"publication","summary":"Most of the existing state of the art sentiment classification techniques involve the use of pre-trained embeddings. This paper postulates a generalized representation that collates training on multiple datasets using a Multi-task learning framework. We incorporate publicly available, pre-trained embeddings with Bidirectional LSTM’s to develop the multi-task model. We validate the representations on an independent test Irony dataset that can contain several sentiments within each sample, with an arbitrary distribution. Our experiments show a significant improvement in results as compared to the available baselines for individual datasets on which independent models are trained. Results also suggest superior performance of the representations generated over Irony dataset.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Learning representations for sentiment classification using Multi-task framework","type":"publication"},{"authors":["Sachin Thukral","Hardik Meisheri","Tushar Kataria","Aman Agarwal","Ishan Verma","Arnab Chatterjee","Lipika Dey"],"categories":null,"content":"","date":1535500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535500800,"objectID":"1f7552a2118ce5387e8e5609d51d5251","permalink":"/publication/analyzing_behavioral_trends_in_community_driven_discussion_platforms_like_reddit/","publishdate":"2018-08-29T00:00:00Z","relpermalink":"/publication/analyzing_behavioral_trends_in_community_driven_discussion_platforms_like_reddit/","section":"publication","summary":"The aim of this paper is to present methods to systematically analyze individual and group behavioral patterns observed in community driven discussion platforms like Reddit where users exchange information and views on various topics of current interest. We conduct this study by analyzing the statistical behavior of posts and modeling user interactions around them. We have chosen Reddit as an example, since it has grown exponentially from a small community to one of the biggest social network platforms in the recent times. Due to its large user base and popularity, a variety of behavior is present among users in terms of their activity. Our study provides interesting insights about a large number of inactive posts which fail to gather attention despite their authors exhibiting Cyborg-like behavior to draw attention. We also present interesting insights about short-lived but extremely active posts emulating a phenomenon like Mayfly Buzz. Further, we present methods to find the nature of activity around highly active posts to determine the presence of Limelight hogging activity, if any. We analyzed over 2 million posts and more than 7 million user responses to them during entire 2008 and over 63 million posts and over 608 million user responses to them from August 2014 to July 2015 amounting to two one-year periods, in order to understand how social media space has evolved over the years.","tags":["Behavioral Analysis","Reddit"],"title":"Analyzing Behavioral Trends in Community Driven Discussion Platforms Like Reddit","type":"publication"},{"authors":["Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1528156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528156800,"objectID":"56037c21d931c77ea5c66596ae66940e","permalink":"/publication/learning-robust-representations-using-multi-attention-architecture/","publishdate":"2018-06-05T00:00:00Z","relpermalink":"/publication/learning-robust-representations-using-multi-attention-architecture/","section":"publication","summary":"This paper presents system description of our submission to the SemEval-2018 task-1, Affect in tweets for the English language. We combine three different features generated using deep learning models and traditional methods in support vector machines to create a unified ensemble system. A robust representation of a tweet is learned using a multi-attention based architecture which uses a mixture of different pre-trained embeddings. In addition to this analysis of different features is also presented. Our system ranked 2nd, 5th, and 7th in different subtasks among 75 teams.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"TCS Research at SemEval-2018 Task 1 Learning Robust Representations using Multi-Attention Architecture","type":"publication"},{"authors":null,"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"909743c8bcfa448b85d409f68a5b00ed","permalink":"/project/reinforcement-learning-in-supply-chain-optimization/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/project/reinforcement-learning-in-supply-chain-optimization/","section":"project","summary":"Applicability of reinforcement learning (RL) algorithms to a class of problems rarely addressed in machine learning literature, involving the control of a dynamic system with high-dimensional control inputs (actions).","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Reinforcement Learning in Supply Chain Optimization","type":"project"},{"authors":null,"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"385b32a5a393666d3cfecf115419d29f","permalink":"/project/pommerman/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/pommerman/","section":"project","summary":"The Pommerman environment is based on the classic Nintendo console game, Bomberman.","tags":["Reinforcement Learning","Games","Pommerman","Deep Learning"],"title":"Pommerman","type":"project"},{"authors":["Hardik Meisheri","Nagaraj Ramrao","Suman K Mitra"],"categories":null,"content":"","date":1519516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519516800,"objectID":"c9ddc0911725e8168cb1b99ae4c071dc","permalink":"/publication/multiclass_common_spatial_pattern_for_eeg_based_brain_computer_interface_with_adaptive_learning_classifier/","publishdate":"2018-02-25T00:00:00Z","relpermalink":"/publication/multiclass_common_spatial_pattern_for_eeg_based_brain_computer_interface_with_adaptive_learning_classifier/","section":"publication","summary":"In Brain Computer Interface (BCI), data generated from Electroencephalogram (EEG) is non-stationary with low signal to noise ratio and contaminated with artifacts. Common Spatial Pattern (CSP) algorithm has been proved to be effective in BCI for extracting features in motor imagery tasks, but it is prone to overfitting. Many algorithms have been devised to regularize CSP for two class problem, however they have not been effective when applied to multiclass CSP. Outliers present in data affect extracted CSP features and reduces performance of the system. In addition to this non-stationarity present in the features extracted from the CSP present a challenge in classification. We propose a method to identify and remove artifact present in the data during pre-processing stage, this helps in calculating eigenvectors which in turn generates better CSP features. To handle the non-stationarity, Self-Regulated Interval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the literature for two class EEG classification problem. This paper extends the SRIT2NFIS to multiclass using Joint Approximate Diagonalization (JAD). The results on standard data set from BCI competition IV shows significant increase in the accuracies from the current state of the art methods for multiclass classification.","tags":["BCI","Neuro-Fuzzy"],"title":"Multiclass Common Spatial Pattern for EEG based Brain Computer Interface with Adaptive Learning Classifier","type":"publication"},{"authors":["Hardik Meisheri","Kunal Ranjan","Lipika Dey"],"categories":null,"content":"","date":1510963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510963200,"objectID":"f2e131ce10c51d0a4fa1c633cb93c738","permalink":"/publication/sentiment_extraction_from_consumer-generated_noisy_short_texts/","publishdate":"2017-11-18T00:00:00Z","relpermalink":"/publication/sentiment_extraction_from_consumer-generated_noisy_short_texts/","section":"publication","summary":"Sentiment analysis or recognizing emotions from short and noisy text from social networks such as twitter has been a challenging task. Most of the existing models use word level embeddings for the final classification of the sentiments. This paper proposes a novel representation of short text derived from a combination of word embeddings and character embeddings using Bidirectional LSTM (BiLSTM). Along with this, we use attention mechanism that learns to focus on sentiment specific words. Robust representation of short text can be applied for sentiment classification as well as predicting intensity of sentiments. This paper presents evaluation of proposed model on classification as well as regression dataset. Results show significant improvement in results as compared to baselines of respective datasets.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Sentiment extraction from Consumer-generated noisy short texts","type":"publication"},{"authors":["Hardik Meisheri","Rupsa Saha","Priyanka Sinha","Lipika Dey"],"categories":null,"content":"","date":1505001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505001600,"objectID":"c1af367246db568f8d26bae0a70d0432","permalink":"/publication/textmining_at_emoint-2017_a_deep_learning_approach_to_sentiment_intensity_scoring_of_english_tweets/","publishdate":"2017-09-10T00:00:00Z","relpermalink":"/publication/textmining_at_emoint-2017_a_deep_learning_approach_to_sentiment_intensity_scoring_of_english_tweets/","section":"publication","summary":"This paper describes our approach to the Emotion Intensity shared task. A parallel architecture of Convolutional Neural Network (CNN) and Long short term memory networks (LSTM) alongwith two sets of features are extracted which aid the network in judging emotion intensity. Experiments on different models and various features sets are described and analysis on results has also been presented.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Textmining at EmoInt-2017: A Deep Learning Approach to Sentiment Intensity Scoring of English Tweets","type":"publication"},{"authors":["Ishna Verma","Lipika Dey","Hardik Meisheri"],"categories":null,"content":"","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"80fb2f63c2e23138a2b1d1623614d3c2","permalink":"/publication/stock_paper/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/publication/stock_paper/","section":"publication","summary":"The impact of different types of events reported in News articles on stock market is a widely accepted phenomenon. Market analysts rely heavily on technology to combine data from different sources and generate appropriate insights for predicting stock movements. With plethora of sources reporting news on plentitude of events happening across the world, a combination of text mining techniques and predictive technologies can play a significant role in this arena. In this paper we have presented methodologies to identify and quantify the presence of different types of information that can affect the market from a multitude of web sources, and finally use the information for predicting stock movement direction. We propose the use of PESTEL factors to categorize market-impacting information. We have analyzed large volumes of past available data using Granger causality to understand how these categories impact the market. We propose a paragraph-vector based information classification mechanism. We also present Long-Short term memory Network (LSTM) based prediction model to investigate the prediction capabilities of the information components. The proposed system outperforms state of the art linear SVM on data from different stock indices.","tags":["NLP","Deep Learning"],"title":"Detecting, quantifying and accessing impact of news events on Indian stock indices","type":"publication"},{"authors":null,"categories":null,"content":"","date":1494806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494806400,"objectID":"484c5bd3a4312256c910e9e600170fe0","permalink":"/project/sentiment-analysis-of-short-text/","publishdate":"2017-05-15T00:00:00Z","relpermalink":"/project/sentiment-analysis-of-short-text/","section":"project","summary":"Sentiment analysis or recognizing emotions from short and noisy text typically from social networks such as Twitter.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Sentiment Analysis of Short text","type":"project"},{"authors":null,"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"c72b37aeed7d0a6bc9b8ec211d95b966","permalink":"/project/behavioral-analysis-of-social-media-posts/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/behavioral-analysis-of-social-media-posts/","section":"project","summary":"To analyze the patterns of individual and group behaviour observed in community-driven discussion platforms like Reddit.","tags":["Behavioral Analysis","Reddit"],"title":"Behavioral Analysis of Social media posts","type":"project"},{"authors":null,"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"769b8a155fbe8e3196ca04777a659bca","permalink":"/project/undergrad-thesis/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/undergrad-thesis/","section":"project","summary":"In this project, we aim to develop a fully functional prototype of motorized wheelchair which can be controlled by speech, joystick and neck movement.","tags":["Embedded Systems","Speech-Recognition","Micro-controllers"],"title":"Undergrad Thesis - Smart Wheel Chair","type":"project"},{"authors":null,"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"dedc1e3fbd3eb71e57b0fb48abc5773e","permalink":"/project/news-aggreation-and-its-effect-over-stock-and-market/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/project/news-aggreation-and-its-effect-over-stock-and-market/","section":"project","summary":"The impact of different types of events reported in News articles on stock market is a widely accepted phenomenon. Market analysts rely heavily on technology to combine data from different sources and generate appropriate insights for predicting stock movements.","tags":["NLP","Deep Learning"],"title":"News aggreation and its effect over Stock and market","type":"project"},{"authors":["Hardik Meisheri","Nagaraj Ramrao","Suman K Mitra"],"categories":null,"content":"","date":1473033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473033600,"objectID":"1cfbee2efff60ddc6a94eeeeba7a5c69","permalink":"/publication/multiclass_common_spatial_pattern_with_artifacts_removal_methodology_for_eeg_signals/","publishdate":"2016-09-05T00:00:00Z","relpermalink":"/publication/multiclass_common_spatial_pattern_with_artifacts_removal_methodology_for_eeg_signals/","section":"publication","summary":"Common Spatial Pattern (SP) algorithm has been proved to be effective in Brain Computer Interface (BCI) for extracting features from Electroencephalogram (EEG) signals used in motor imagery tasks, but it is vulnerable to noise and the problem of over-fitting. Many algorithms have been devised to regularize CSP for two class problem, however they have not been effective when applied to multiclass CSP. The features extracted using the CSP are non-stationary in nature which increases the difficulty during classification. We propose a method to remove trials that are affected by noise before calculating the CSP. This helps in calculating eigenvectors which generates better CSP. To handle the non-stationarity in the EEG signal, Self-Regulated Interval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the literature for two class EEG classification problem. This paper extends the SRIT2NFIS to Multiclass CSP using Joint Approximate Diagonalization (JAD). The results are presented on standard dataset.","tags":["BCI","Neuro-Fuzzy"],"title":"Multiclass common spatial pattern with artifacts removal methodology for EEG signals","type":"publication"},{"authors":null,"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"26375b4faf48283ee4082101d48bf5d4","permalink":"/project/brain-computer-interface/","publishdate":"2015-05-01T00:00:00Z","relpermalink":"/project/brain-computer-interface/","section":"project","summary":"Brain Computer Interface (BCI) provides a pathway for communication from humans to computer without any muscular activity.","tags":["Brain Computer Interface"],"title":"Brain Computer Interface","type":"project"}]